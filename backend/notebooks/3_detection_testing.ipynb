{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergency Vehicle Detection - Testing and Auto-Annotation\n",
    "\n",
    "This notebook tests the trained YOLOv5 model and demonstrates auto-annotation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "with open('../Dataset/dataset.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load trained model\n",
    "model = torch.load('best_model.pt', map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize normalizer\n",
    "normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_image(image, target_size=640):\n",
    "    \"\"\"Preprocess image for model inference\"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = F.to_pil_image(image)\n",
    "    \n",
    "    # Resize maintaining aspect ratio\n",
    "    w, h = image.size\n",
    "    scale = target_size / max(w, h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    image = F.resize(image, [new_h, new_w])\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image = F.to_tensor(image)\n",
    "    image = normalizer(image)\n",
    "    \n",
    "    return image, (scale, (new_w, new_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_image(image, conf_threshold=0.5):\n",
    "    \"\"\"Detect vehicles in a single image\"\"\"\n",
    "    # Preprocess image\n",
    "    img_tensor, (scale, (w, h)) = preprocess_image(image)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        results = model(img_tensor)\n",
    "    \n",
    "    # Process detections\n",
    "    detections = []\n",
    "    for pred in results.pred[0]:\n",
    "        if pred.conf > conf_threshold:\n",
    "            # Scale coordinates back to original image size\n",
    "            x1, y1, x2, y2 = pred[:4] / scale\n",
    "            conf = pred.conf\n",
    "            cls = int(pred.cls)\n",
    "            \n",
    "            detection = {\n",
    "                'class': config['names'][cls],\n",
    "                'confidence': float(conf),\n",
    "                'bbox': [float(x1), float(y1), float(x2), float(y2)]\n",
    "            }\n",
    "            detections.append(detection)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def draw_detections(image, detections):\n",
    "    \"\"\"Draw detection results on the image\"\"\"\n",
    "    colors = {\n",
    "        'Ambulance': (255, 0, 0),    # Red\n",
    "        'Fire Engine': (0, 0, 255),  # Blue\n",
    "        'Police': (0, 255, 0),       # Green\n",
    "        'Non Emergency': (128, 128, 128)  # Gray\n",
    "    }\n",
    "    \n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = np.array(det['bbox']).astype(int)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        color = colors.get(det['class'], (0, 255, 0))\n",
    "        \n",
    "        # Draw box\n",
    "        cv2.rectangle(img_copy,\n",
    "                     (bbox[0], bbox[1]),\n",
    "                     (bbox[2], bbox[3]),\n",
    "                     color, 2)\n",
    "        \n",
    "        # Draw label with background\n",
    "        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(img_copy,\n",
    "                     (bbox[0], bbox[1] - text_size[1] - 5),\n",
    "                     (bbox[0] + text_size[0], bbox[1]),\n",
    "                     color, -1)\n",
    "        cv2.putText(img_copy, label,\n",
    "                   (bbox[0], bbox[1] - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_annotations(images_dir, output_file, conf_threshold=0.7):\n",
    "    \"\"\"Generate annotations for images using the trained model\"\"\"\n",
    "    annotations = []\n",
    "    images = Path(images_dir).glob('*.jpg')\n",
    "    \n",
    "    for img_path in tqdm(images, desc='Generating annotations'):\n",
    "        image = cv2.imread(str(img_path))\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        detections = detect_image(image, conf_threshold)\n",
    "        \n",
    "        for det in detections:\n",
    "            bbox = det['bbox']\n",
    "            \n",
    "            # Convert to YOLO format\n",
    "            x_center = (bbox[0] + bbox[2]) / 2 / w\n",
    "            y_center = (bbox[1] + bbox[3]) / 2 / h\n",
    "            width = (bbox[2] - bbox[0]) / w\n",
    "            height = (bbox[3] - bbox[1]) / h\n",
    "            \n",
    "            annotations.append({\n",
    "                'image': img_path.name,\n",
    "                'x_center': x_center,\n",
    "                'y_center': y_center,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'class': list(config['names'].keys())[list(config['names'].values()).index(det['class'])],\n",
    "                'confidence': det['confidence']\n",
    "            })\n",
    "    \n",
    "    # Save annotations\n",
    "    pd.DataFrame(annotations).to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(annotations)} annotations to {output_file}\")\n",
    "    \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on sample images\n",
    "test_images = ['../Dataset/val/images/ambulance1.jpg', \n",
    "               '../Dataset/val/images/fire_engine2.jpg']\n",
    "\n",
    "for img_path in test_images:\n",
    "    print(f\"\\nProcessing {img_path}...\")\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read image at {img_path}\")\n",
    "        continue\n",
    "        \n",
    "    detections = detect_image(img)\n",
    "    img_with_detections = draw_detections(img, detections)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(img_with_detections, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Detections for {Path(img_path).name}')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Detections:\")\n",
    "    for det in detections:\n",
    "        print(f\"- {det['class']}: {det['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test video detection\n",
    "def process_video(video_path, output_path=None, display=True):\n",
    "    \"\"\"Process video for vehicle detection\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video at {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Initialize video writer if output path is provided\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    try:\n",
    "        with tqdm(total=total_frames, desc=\"Processing video\") as pbar:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Process every 2nd frame for speed\n",
    "                if frame_count % 2 == 0:\n",
    "                    detections = detect_image(frame)\n",
    "                    frame_with_detections = draw_detections(frame, detections)\n",
    "\n",
    "                    if output_path:\n",
    "                        out.write(frame_with_detections)\n",
    "\n",
    "                    if display:\n",
    "                        cv2.imshow('Emergency Vehicle Detection', frame_with_detections)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "                frame_count += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        if output_path:\n",
    "            out.release()\n",
    "        if display:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "# Test on video\n",
    "video_path = '../Dataset/test_video.mp4'\n",
    "output_path = '../Dataset/output_video.mp4'\n",
    "process_video(video_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate new annotations for a directory of images\n",
    "new_images_dir = '../Dataset/new_images'\n",
    "output_annotations = '../Dataset/new_annotations.csv'\n",
    "\n",
    "# Uncomment to generate annotations\n",
    "# annotations = generate_annotations(new_images_dir, output_annotations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
